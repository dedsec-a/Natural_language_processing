{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now demonstrate how do we use stop words dict and remove that stop words from the given corpus in order to remoce them and them apply the other popcess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"Today, all of us do, by our presence here, and by our celebrations in other parts of our country and the world, confer glory and hope to newborn liberty. Out of the experience of an extraordinary human disaster that lasted too long, must be born a society of which all humanity will be proud. Our daily deeds as ordinary South Africans must produce an actual South African reality that will reinforce humanity's belief in justice, strengthen its confidence in the nobility of the human soul, and sustain all our hopes for a glorious life for all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reply\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\reply\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing And Downloading the Stop Words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'll\", 'not', 'shan', 'did', 'should', 'me', 'in', \"won't\", 'if', \"couldn't\", 'be', 'then', 'into', 'mightn', \"shan't\", 'because', 'they', 'at', 'm', 'himself', 'or', 'having', \"wasn't\", 'what', 'hadn', 'out', 'until', 'hers', 'those', 'he', 'too', 'hasn', \"it's\", 'is', \"didn't\", 'were', 'any', 'was', 'wasn', 'isn', 'its', 'by', 'before', \"you'd\", 'her', 'ours', 'them', 'it', 'yourself', 'why', 'll', \"should've\", 'further', 'she', \"doesn't\", 'off', 'now', 'd', 'our', \"don't\", 'on', 'between', 'whom', 'have', 'through', \"hasn't\", \"shouldn't\", 'couldn', 'their', 'some', 'here', 'do', 'we', 'a', 'the', 'that', \"mightn't\", 'against', 'has', 'after', 'up', 'so', 'needn', 'his', 'won', 'who', 'mustn', 'can', 'am', 'i', 'only', 'o', 'below', 'doesn', 'same', 'themselves', 'to', \"weren't\", 'haven', \"wouldn't\", 'herself', 'as', 'and', 'yours', 'being', 's', \"aren't\", 'all', 'for', \"mustn't\", 'does', 'under', 'yourselves', 'while', 'doing', 'over', 'don', 'but', 'will', 'him', 'y', 'very', \"isn't\", 'just', 'shouldn', 'these', 'from', 'nor', 'ain', 'are', 'how', 'ma', 'wouldn', \"she's\", 'theirs', 'myself', 'down', 'your', 'you', 'with', 'most', \"haven't\", 'few', 're', \"hadn't\", 'this', 'when', \"you've\", 'again', 'each', 'ourselves', 'been', 'where', \"you're\", 'more', 'above', 'other', 't', \"needn't\", 'my', 'had', 'once', 'an', 'there', 'of', 'itself', 'such', 'both', 'about', 'no', 'during', 'weren', 'aren', 'than', \"that'll\", 'which', 'didn', 'own', 've'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops) # here we have a good amount of stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, all of us do, by our presence here, and by our celebrations in other parts of our country and the world, confer glory and hope to newborn liberty. Out of the experience of an extraordinary human disaster that lasted too long, must be born a society of which all humanity will be proud. Our daily deeds as ordinary South Africans must produce an actual South African reality that will reinforce humanity's belief in justice, strengthen its confidence in the nobility of the human soul, and sustain all our hopes for a glorious life for all\n"
     ]
    }
   ],
   "source": [
    "print(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning it into words tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'all', 'of', 'us', 'do', ',', 'by', 'our', 'presence', 'here', ',', 'and', 'by', 'our', 'celebrations', 'in', 'other', 'parts', 'of', 'our', 'country', 'and', 'the', 'world', ',', 'confer', 'glory', 'and', 'hope', 'to', 'newborn', 'liberty', '.', 'Out', 'of', 'the', 'experience', 'of', 'an', 'extraordinary', 'human', 'disaster', 'that', 'lasted', 'too', 'long', ',', 'must', 'be', 'born', 'a', 'society', 'of', 'which', 'all', 'humanity', 'will', 'be', 'proud', '.', 'Our', 'daily', 'deeds', 'as', 'ordinary', 'South', 'Africans', 'must', 'produce', 'an', 'actual', 'South', 'African', 'reality', 'that', 'will', 'reinforce', 'humanity', \"'s\", 'belief', 'in', 'justice', ',', 'strengthen', 'its', 'confidence', 'in', 'the', 'nobility', 'of', 'the', 'human', 'soul', ',', 'and', 'sustain', 'all', 'our', 'hopes', 'for', 'a', 'glorious', 'life', 'for', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'us', ',', 'presence', ',', 'celebrations', 'parts', 'country', 'world', ',', 'confer', 'glory', 'hope', 'newborn', 'liberty', '.', 'Out', 'experience', 'extraordinary', 'human', 'disaster', 'lasted', 'long', ',', 'must', 'born', 'society', 'humanity', 'proud', '.', 'Our', 'daily', 'deeds', 'ordinary', 'South', 'Africans', 'must', 'produce', 'actual', 'South', 'African', 'reality', 'reinforce', 'humanity', \"'s\", 'belief', 'justice', ',', 'strengthen', 'confidence', 'nobility', 'human', 'soul', ',', 'sustain', 'hopes', 'glorious', 'life']\n"
     ]
    }
   ],
   "source": [
    "wordsFiltered = [w for w in words if w not in stops]\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now applying the Stemming and Lematazation on the Filtered Texts\n",
    "from nltk.stem import SnowballStemmer\n",
    "sc = SnowballStemmer(language='english')\n",
    "for word in wordsFiltered :\n",
    "  stemmed_words = [sc.stem(word.lower()) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'us',\n",
       " 'do',\n",
       " ',',\n",
       " 'by',\n",
       " 'our',\n",
       " 'presenc',\n",
       " 'here',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'our',\n",
       " 'celebr',\n",
       " 'in',\n",
       " 'other',\n",
       " 'part',\n",
       " 'of',\n",
       " 'our',\n",
       " 'countri',\n",
       " 'and',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'confer',\n",
       " 'glori',\n",
       " 'and',\n",
       " 'hope',\n",
       " 'to',\n",
       " 'newborn',\n",
       " 'liberti',\n",
       " '.',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'experi',\n",
       " 'of',\n",
       " 'an',\n",
       " 'extraordinari',\n",
       " 'human',\n",
       " 'disast',\n",
       " 'that',\n",
       " 'last',\n",
       " 'too',\n",
       " 'long',\n",
       " ',',\n",
       " 'must',\n",
       " 'be',\n",
       " 'born',\n",
       " 'a',\n",
       " 'societi',\n",
       " 'of',\n",
       " 'which',\n",
       " 'all',\n",
       " 'human',\n",
       " 'will',\n",
       " 'be',\n",
       " 'proud',\n",
       " '.',\n",
       " 'our',\n",
       " 'daili',\n",
       " 'deed',\n",
       " 'as',\n",
       " 'ordinari',\n",
       " 'south',\n",
       " 'african',\n",
       " 'must',\n",
       " 'produc',\n",
       " 'an',\n",
       " 'actual',\n",
       " 'south',\n",
       " 'african',\n",
       " 'realiti',\n",
       " 'that',\n",
       " 'will',\n",
       " 'reinforc',\n",
       " 'human',\n",
       " \"'s\",\n",
       " 'belief',\n",
       " 'in',\n",
       " 'justic',\n",
       " ',',\n",
       " 'strengthen',\n",
       " 'it',\n",
       " 'confid',\n",
       " 'in',\n",
       " 'the',\n",
       " 'nobil',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'soul',\n",
       " ',',\n",
       " 'and',\n",
       " 'sustain',\n",
       " 'all',\n",
       " 'our',\n",
       " 'hope',\n",
       " 'for',\n",
       " 'a',\n",
       " 'glorious',\n",
       " 'life',\n",
       " 'for',\n",
       " 'all']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Lametazation \n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "wd = WordNetLemmatizer()\n",
    "for lemma in wordsFiltered :lemmatized_words = [wd.lemmatize(word) for word in wordsFiltered]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " ',',\n",
       " 'u',\n",
       " ',',\n",
       " 'presence',\n",
       " ',',\n",
       " 'celebration',\n",
       " 'part',\n",
       " 'country',\n",
       " 'world',\n",
       " ',',\n",
       " 'confer',\n",
       " 'glory',\n",
       " 'hope',\n",
       " 'newborn',\n",
       " 'liberty',\n",
       " '.',\n",
       " 'Out',\n",
       " 'experience',\n",
       " 'extraordinary',\n",
       " 'human',\n",
       " 'disaster',\n",
       " 'lasted',\n",
       " 'long',\n",
       " ',',\n",
       " 'must',\n",
       " 'born',\n",
       " 'society',\n",
       " 'humanity',\n",
       " 'proud',\n",
       " '.',\n",
       " 'Our',\n",
       " 'daily',\n",
       " 'deed',\n",
       " 'ordinary',\n",
       " 'South',\n",
       " 'Africans',\n",
       " 'must',\n",
       " 'produce',\n",
       " 'actual',\n",
       " 'South',\n",
       " 'African',\n",
       " 'reality',\n",
       " 'reinforce',\n",
       " 'humanity',\n",
       " \"'s\",\n",
       " 'belief',\n",
       " 'justice',\n",
       " ',',\n",
       " 'strengthen',\n",
       " 'confidence',\n",
       " 'nobility',\n",
       " 'human',\n",
       " 'soul',\n",
       " ',',\n",
       " 'sustain',\n",
       " 'hope',\n",
       " 'glorious',\n",
       " 'life']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we did all the inital steps for NLP which includes \n",
    "1. Tokenization \n",
    "2. lowercasing\n",
    "3. removing of stopWords\n",
    "4. Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
